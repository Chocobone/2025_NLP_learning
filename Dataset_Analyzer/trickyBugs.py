# ----------------------------
# í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
# ----------------------------
import os
import json
import math
import re
import shutil
import subprocess
from pathlib import Path
from itertools import combinations
from typing import List, Dict

# ----------------------------
# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬
# ----------------------------
import torch
import pandas as pd
from transformers import AutoModelForCausalLM, AutoTokenizer

# ----------------------------------------------------
# 1. ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ í•œ ë²ˆë§Œ ë¡œë“œí•˜ë„ë¡ ì „ì—­ ë³€ìˆ˜ë¡œ ì„¤ì •
# ----------------------------------------------------
print("Loading model and tokenizer...")
# MODEL_IDë¥¼ Qwen Coder AWQ ëª¨ë¸ë¡œ ë³€ê²½
MODEL_ID = "Qwen/Qwen2.5-Coder-32B-Instruct-AWQ" 
MODEL = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    torch_dtype="auto",        # AWQ ëª¨ë¸ë„ autoë¡œ ì„¤ì •í•˜ë©´ ë©ë‹ˆë‹¤.
    trust_remote_code=True,  # Qwen ëª¨ë¸ ì‹¤í–‰ ì‹œ í•„ìš”í•©ë‹ˆë‹¤.
    device_map="auto"        # GPU VRAMì— ë§ê²Œ ëª¨ë¸ì„ ìë™ ë°°ì¹˜í•©ë‹ˆë‹¤.
)
TOKENIZER = AutoTokenizer.from_pretrained(MODEL_ID)
print("Model and tokenizer loaded successfully! ğŸš€")

# ----------------------------------------------------
# 2. LLM í˜¸ì¶œì„ ìœ„í•œ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í•¨ìˆ˜ ì •ì˜
# ----------------------------------------------------

def extract_code(text: str) -> str:
    """
    ëª¨ë¸ì˜ ì „ì²´ ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ ë§ˆì§€ë§‰ C++ ì½”ë“œ ë¸”ë¡ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    if "<think>" in text:
        print("[Debug] '<think>' tag found. Skipping markdown code block search.")
        start_match = re.search(r"#include", text)
        if not start_match:
            print("[Warning] No '#include' found.")
            return text

        start_index = start_match.start()
        think_match = re.search(r"<think>", text)
        search_area_end_index = think_match.start()

        search_area = text[start_index:search_area_end_index]
        last_brace_index_in_area = search_area.rfind("}")

        if last_brace_index_in_area != -1:
            print("[Debug] Extracted code from '#include' to last '}' before '<think>'.")
            return search_area[:last_brace_index_in_area + 1].strip()
        
        print("[Warning] Could not find closing '}' before <think>.")
        return search_area.strip()

    pattern = r"```(?:cpp)?\s*(.*?)\s*```"
    matches = re.findall(pattern, text, re.DOTALL)

    if matches:
        print(f"[Debug] Found {len(matches)} code blocks. Extracting the last one.")
        return matches[-1].strip()

    print("[Warning] No code block found at all.")
    return text


def generate_code_response(
    messages: List[Dict[str, str]],
    max_new_tokens: int = 3500
) -> str:
    """
    ì½”ë“œ ìƒì„± ì „ìš©: ì£¼ì–´ì§„ ë©”ì‹œì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ LLMì˜ ì‘ë‹µì„ ìƒì„±í•˜ê³  'ì½”ë“œ'ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    do_sample=Falseë¡œ ì„¤ì •í•˜ì—¬ ê²°ì •ë¡ ì  ìƒì„± (temperature ë¬´ì‹œë¨)
    """
    with torch.no_grad():
        inputs = TOKENIZER.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt",
        ).to(MODEL.device)

        outputs = MODEL.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=False,  # âœ… ê²°ì •ë¡ ì  ìƒì„± - temperature ë¬´ì‹œë¨
            eos_token_id=TOKENIZER.eos_token_id,
            pad_token_id=TOKENIZER.eos_token_id
        )

        response_ids = outputs[:, inputs["input_ids"].shape[-1]:]
        full_response_text = TOKENIZER.batch_decode(response_ids, skip_special_tokens=True)[0]
    
    torch.cuda.empty_cache()
    extracted_code = extract_code(full_response_text)
    
    return extracted_code


def generate_text_response(
    messages: List[Dict[str, str]],
    max_new_tokens: int = 512
) -> str:
    """
    ì¼ë°˜ í…ìŠ¤íŠ¸ ìƒì„± ì „ìš©: í…ŒìŠ¤íŠ¸ ì…ë ¥ ë“±ì„ ìƒì„±í•  ë•Œ ì‚¬ìš©.
    ì½”ë“œ ì¶”ì¶œ ì—†ì´ ì›ë³¸ ì‘ë‹µì„ ê·¸ëŒ€ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    do_sample=Falseë¡œ ì„¤ì •í•˜ì—¬ ê²°ì •ë¡ ì  ìƒì„±
    """
    with torch.no_grad():
        inputs = TOKENIZER.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt",
        ).to(MODEL.device)

        outputs = MODEL.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=False,  # âœ… ê²°ì •ë¡ ì  ìƒì„±
            eos_token_id=TOKENIZER.eos_token_id,
            pad_token_id=TOKENIZER.eos_token_id
        )

        response_ids = outputs[:, inputs["input_ids"].shape[-1]:]
        full_response_text = TOKENIZER.batch_decode(response_ids, skip_special_tokens=True)[0]
    
    torch.cuda.empty_cache()
    
    return full_response_text


def _read_text_if_exists(path: Path | None) -> str:
    """íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ í…ìŠ¤íŠ¸ë¥¼ ì½ê³ , ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if path and path.exists():
        try:
            return path.read_text(encoding="utf-8")
        except Exception as e:
            print(f"[Warning] Could not read file {path}: {e}")
            return ""
    return ""


def load_trickybugs_data_revised(base_path: str, lang: str = None) -> Dict:
    """
    TrickyBugs íŒŒì¼ êµ¬ì¡°ì— ë§ì¶° ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    print(f"Loading TrickyBugs data...")

    base = Path(base_path)
    problems_dir = base / "problems"

    if not problems_dir.exists():
        raise FileNotFoundError(f"Problems directory not found at: {problems_dir}")

    loaded_problems = {}
    pid_dirs = sorted([p for p in problems_dir.iterdir() if p.is_dir()])

    for pid_dir in pid_dirs:
        pid = pid_dir.name
        buggy_base = pid_dir / "buggy_programs"
        if not buggy_base.exists():
            print(f"[Warning] No buggy_programs for {pid}")
            continue

        if lang:
            selected_lang_dir = buggy_base / lang
            if not selected_lang_dir.exists():
                print(f"[Warning] No buggy_programs/{lang} for {pid}")
                continue
        else:
            lang_dirs = [d for d in buggy_base.iterdir() if d.is_dir()]
            if not lang_dirs:
                print(f"[Warning] No language directories in buggy_programs for {pid}")
                continue
            selected_lang_dir = lang_dirs[0]
            lang = selected_lang_dir.name

        source_files = list(selected_lang_dir.glob(f"*.{lang}"))
        if not source_files:
            print(f"[Warning] No source files found in: {selected_lang_dir}")
            continue

        put_path = source_files[0]
        put_code = _read_text_if_exists(put_path)
        if not put_code:
            continue

        spec_path = pid_dir / "problem_description.txt"
        spec_text = _read_text_if_exists(spec_path)

        meta_path = pid_dir / "metainfo.json"
        meta_data = {}
        if meta_path.exists():
            try:
                meta_data = json.loads(meta_path.read_text(encoding="utf-8"))
            except Exception as e:
                print(f"[Warning] Failed to read {meta_path}: {e}")

        loaded_problems[pid] = {
            "language": lang,
            "spec": spec_text,
            "put_code": put_code,
            "meta": {
                "pid_path": str(pid_dir),
                "put_path": str(put_path),
                "metainfo": meta_data
            }
        }

    print(f"âœ… TrickyBugs ë¡œë”© ì™„ë£Œ: {len(loaded_problems)}ê°œ ë¬¸ì œ")
    return loaded_problems


def create_variants(problems, output_base_path, k: int = 1):
    """
    TrickyBugs ë¬¸ì œ ë°ì´í„°ì—ì„œ ê° ì½”ë“œì˜ ìˆ˜ì • ë²„ì „ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    genprog_prompt = """You are a professional coding competition participant, skilled at identifying bugs and logic flaws in code.
You will receive a description of a coding problem, and a piece of code attempting to solve the problem.
Your task is to repair the code.

Your response MUST contain only the complete C++ code, formatted inside a single markdown code block.
Start your response IMMEDIATELY with ```cpp and end your response IMMEDIATELY with ```.
Do not provide any text, explanation, or reasoning before or after the code block.

**PROBLEM DESCRIPTION**:
{pro_des}

**CODE**:
{code}
"""

    for pid, problem in problems.items():
        print(f"\n--- Processing PID: {pid} ---")
        variant_dir = Path(output_base_path) / "GenProgs" / pid
        variant_dir.mkdir(parents=True, exist_ok=True)

        for i in range(1, k + 1):
            print(f"ğŸ¤– Generating repaired program variant #{i}...")

            prog_messages = [
                {
                    "role": "user",
                    "content": genprog_prompt.format(
                        pro_des=problem["spec"],
                        code=problem["put_code"]
                    )
                }
            ]

            # âœ… ìˆ˜ì •: generate_code_response ì§ì ‘ í˜¸ì¶œ, 1280 í† í° ì‚¬ìš©
            variant_code = generate_code_response(prog_messages, max_new_tokens=1280)

            variant_file = variant_dir / f"variant_{i}.{problem['language']}"
            with open(variant_file, "w", encoding="utf-8") as f:
                f.write(variant_code)

            print(f"   âœ… Saved repaired code to: {variant_file}")

    print(f"\nğŸ¯ ëª¨ë“  ë¬¸ì œì˜ {k}ê°œ ë³€í˜• ì½”ë“œ ìƒì„± ì™„ë£Œ!")


def generate_buggy_test_inputs(problems, output_base_path, num_inputs: int = 10):
    """
    LLMì—ê²Œ ê° ë¬¸ì œ ëª…ì„¸ì™€ ë²„ê·¸ ì½”ë“œë¥¼ ì „ë‹¬í•˜ì—¬ í…ŒìŠ¤íŠ¸ ì…ë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    output_base = Path(output_base_path) / "chat_generated_inputs"
    output_base.mkdir(parents=True, exist_ok=True)

    for pid, problem in problems.items():
        print(f"\n--- Generating test inputs for PID: {pid} ---")
        
        pid_dir = output_base / pid
        pid_dir.mkdir(parents=True, exist_ok=True)
        
        prompt = f"""**INSTRUCTION**:
You are a professional software testing engineer. You will get a problem description of a coding problem, and a piece of code attempting to solve the problem. 
Please generate {num_inputs} diverse and corner test inputs that could potentially trigger bugs.
Every input must adhere to the constraints and format mentioned in the problem description.
Please reply with ONLY the generated input without any other content, use the following template:
INPUT1:
(content of the 1st generated test input)
INPUT2:
(content of the 2nd generated test input)
...
INPUT{num_inputs}:
(content of the {num_inputs}-th generated test input)

**PROBLEM DESCRIPTION**:
{problem["spec"]}

**CODE**:
{problem["put_code"]}
"""
        messages = [{"role": "user", "content": prompt}]
        
        # âœ… ìˆ˜ì •: generate_text_response ì§ì ‘ í˜¸ì¶œ, 512 í† í° ì‚¬ìš©
        response = generate_text_response(messages, max_new_tokens=512)

        pattern = r"INPUT\d+:\s*(.*?)\s*(?=INPUT\d+:|$)"
        matches = re.findall(pattern, response, flags=re.DOTALL)

        if not matches:
            print(f"  âŒ No inputs generated for PID {pid}")
            continue

        for idx, input_content in enumerate(matches[:num_inputs], start=1):
            input_file = pid_dir / f"chatGenInput_{idx}.in"
            input_file.write_text(input_content.strip(), encoding="utf-8")
            print(f"  âœ… Saved {input_file.name}")

    print("\nğŸ¯ ëª¨ë“  ë¬¸ì œì˜ í…ŒìŠ¤íŠ¸ ì…ë ¥ ìƒì„± ì™„ë£Œ!")


def verify_variants(problems, output_base_path):
    """
    ìƒì„±ëœ variant ì½”ë“œ ì¤‘ ì›ë³¸ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ ëª¨ë‘ í†µê³¼í•˜ëŠ” ì½”ë“œë§Œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    genprogs_base = Path(output_base_path) / "GenProgs"
    verified_base = Path(output_base_path) / "GenProgsVerified"
    verified_base.mkdir(parents=True, exist_ok=True)

    for pid, problem in problems.items():
        lang = problem["language"]
        print(f"\n--- Verifying PID: {pid} ({lang}) ---")
        
        variant_dir = genprogs_base / pid
        if not variant_dir.exists():
            print(f"[Warning] No variants found for {pid}")
            continue

        test_dir = Path(problem["meta"]["pid_path"]) / "original_test_cases"
        if not test_dir.exists():
            print(f"[Warning] No original_test_cases for {pid}")
            continue

        variant_files = list(variant_dir.glob(f"*.{lang}"))
        for variant_file in variant_files:
            print(f"Checking {variant_file.name} ...")
            all_passed = True

            if lang == "cpp":
                exec_file = variant_file.parent / "tmp_exec"
                try:
                    compile_cmd = ["g++", "-std=c++17", str(variant_file), "-o", str(exec_file)]
                    subprocess.run(compile_cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                except subprocess.CalledProcessError as e:
                    print(f"  âŒ Compile failed: {e}")
                    all_passed = False
                    continue
            else:
                exec_file = variant_file

            input_files = sorted(test_dir.glob("*.in"))
            for in_file in input_files:
                out_file = test_dir / (in_file.stem + ".out")
                if not out_file.exists():
                    continue

                try:
                    if lang == "cpp":
                        result = subprocess.run([str(exec_file)], input=in_file.read_bytes(),
                                                stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=5)
                    else:
                        result = subprocess.run(["python3", str(exec_file)], input=in_file.read_bytes(),
                                                stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=5)

                    expected_output = out_file.read_bytes()
                    if result.stdout.strip() != expected_output.strip():
                        all_passed = False
                        print(f"  âŒ Test failed: {in_file.name}")
                        break
                except subprocess.TimeoutExpired:
                    all_passed = False
                    print(f"  âŒ Test timed out: {in_file.name}")
                    break

            if all_passed:
                pid_verified_dir = verified_base / pid
                pid_verified_dir.mkdir(parents=True, exist_ok=True)
                shutil.copy(variant_file, pid_verified_dir / variant_file.name)
                print(f"  âœ… Passed all tests: {variant_file.name}")

            if lang == "cpp" and exec_file.exists():
                exec_file.unlink()

    print("\nğŸ¯ ëª¨ë“  ë¬¸ì œì˜ Verified variants ì²˜ë¦¬ ì™„ë£Œ!")


def run_code(lang:str, code_path:Path, input_bytes:bytes, timeout:int=5) -> bytes:
    """C++ ë˜ëŠ” Python ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê³  stdout ë°˜í™˜"""
    if lang == "cpp":
        exec_file = code_path.parent / "tmp_exec"
        try:
            subprocess.run(["g++", "-std=c++17", str(code_path), "-o", str(exec_file)],
                           check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        except subprocess.CalledProcessError as e:
            print(f"[Compile Error] {code_path}: {e}")
            return b""
        try:
            result = subprocess.run([str(exec_file)], input=input_bytes,
                                    stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=timeout)
        except subprocess.TimeoutExpired:
            return b"TIMEOUT"
        finally:
            if exec_file.exists():
                exec_file.unlink()
        return result.stdout
    elif lang == "python":
        try:
            result = subprocess.run(["python3", str(code_path)], input=input_bytes,
                                    stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=timeout)
        except subprocess.TimeoutExpired:
            return b"TIMEOUT"
        return result.stdout
    else:
        raise ValueError(f"Unsupported language: {lang}")


def task_oracle(problems:Dict, inputs_base_path:str, lang:str):
    """
    ê° ë¬¸ì œì˜ PUTê³¼ reference ì†”ë£¨ì…˜ì— ëŒ€í•´ LLMì´ ìƒì„±í•œ í…ŒìŠ¤íŠ¸ ì…ë ¥ì„ ì‹¤í–‰í•˜ê³ 
    ì¶œë ¥ ë¹„êµ í›„ íŠ¸ë¦¬ê±° ë°ì´í„°í”„ë ˆì„ ìƒì„±
    
    Returns:
        pd.DataFrame(columns=['pid', 'input_name', 'sol_name', 'out', 'input_valid', 'out_correct'])
    """
    df_rows = []
    inputs_base = Path(inputs_base_path)
    
    for pid, problem in problems.items():
        pid_inputs_dir = inputs_base / pid
        if not pid_inputs_dir.exists():
            print(f"[Warning] No test inputs for {pid}")
            continue
        input_files = sorted(pid_inputs_dir.glob("*.in"))
        
        # ëª¨ë“  ì†”ë£¨ì…˜ (PUT + other) ê²½ë¡œ ìˆ˜ì§‘
        sol_files = [(problem['put_code'], 'put')]
        # GenProgsVerifiedê°€ ìˆë‹¤ë©´ reference ì†”ë£¨ì…˜ ì¶”ê°€
        verified_dir = Path(inputs_base_path).parent / "GenProgsVerified" / pid
        if verified_dir.exists():
            for f in verified_dir.glob(f"*.{lang}"):
                sol_files.append((f, f.stem))
        
        for input_file in input_files:
            input_bytes = input_file.read_bytes()
            for code, sol_name in sol_files:
                if isinstance(code, str):
                    # PUT ì½”ë“œëŠ” ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
                    tmp_file = Path(f"tmp_put_{pid}.{lang}")
                    tmp_file.write_text(code, encoding="utf-8")
                    code_path = tmp_file
                else:
                    code_path = code  # ì´ë¯¸ íŒŒì¼ì¸ reference
                out = run_code(lang, code_path, input_bytes)
                if isinstance(code, str) and tmp_file.exists():
                    tmp_file.unlink()
                input_valid = out not in [b"", b"TIMEOUT"]
                df_rows.append({
                    "pid": pid,
                    "input_name": input_file.name,
                    "sol_name": sol_name,
                    "out": out,
                    "input_valid": input_valid,
                    "out_correct": input_valid  # PUT ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨ ê°€ëŠ¥
                })
    
    df = pd.DataFrame(df_rows)
    return df

combination_cache = {}
def Cnk(n, k):
    if (n, k) in combination_cache:
        return combination_cache[(n, k)]
    result = math.comb(n, k)
    combination_cache[(n, k)] = result
    return result

def get_trigger_df(df:pd.DataFrame,lang:str,method_type:str):
    '''
    method_type: 'dfp' or 'tc'
    lang: 'cpp' or 'python'
    '''
    assert lang in ['cpp','python']
    if method_type!='dfp' and method_type!='tc':
        raise RuntimeError(f"Wrong method_type: {method_type}")

    result_df = pd.DataFrame(columns=['pid', 'input_name','out','sol_names','input_valid','input_valid_byref','out_correct'])

    # A: the PUT
    # B: other programs
    if lang=='cpp':
        A = df[df['sol_name'].str.startswith('sol_')]
        B = df[~df['sol_name'].str.startswith('sol_')]
    elif lang=='python':
        A = df[~df['sol_name'].str.startswith('p0')]
        B = df[df['sol_name'].str.startswith('p0')]
    
    A_outputs = A[['pid', 'input_name', 'out']].drop_duplicates()

    # check B outputs with the same value
    B_grouped = (B.groupby(['pid', 'input_name']))
    totoal_len=len(B_grouped)
    count=0
    for (pid, input_name), group_df in B_grouped:
        count+=1
        if count%1000==0:
            print(f"get_triger: {count}/{totoal_len}")
        group_df=group_df.drop_duplicates(subset=['pid','input_name','sol_name','out'])
        # deduplicate is import when multiple ref_out occur
        # For Example:
        # A[(A['pid']=='p02550') & (A['input_name']=='1_1_1.in.json')]
        # pid	input_name	sol_name	out	is_out_hash	input_valid	number_of_sols	ref_out	is_refout_hash	input_valid_byref	out_correct
        # 121	p02550	1_1_1.in.json	sol_129.out	0	False	0	4	104	False	False	False
        # 122	p02550	1_1_1.in.json	sol_129.out	0	False	0	1	0	False	False	True
        out_values = group_df['out'].values
        unique_out_values = set(out_values)        

        for out_value in unique_out_values:
            
            a_out_df=A_outputs[(A_outputs['pid'] == pid) & (A_outputs['input_name'] == input_name)]
         
            if len(a_out_df)<1 :
                continue
            a_out = A[(A['pid'] == pid) & (A['input_name'] == input_name)]['out'].values[0]
            if out_value==a_out:
                continue
        # get sol_name with the same out
            matching_sol_names = group_df[group_df['out']==out_value]['sol_name'].to_list()
            try:
                input_valid=group_df[group_df['out']==out_value]['input_valid'].to_list()[0]
                input_valid_byref=group_df[group_df['out']==out_value]['input_valid_byref'].to_list()[0]
                out_correct=group_df[group_df['out']==out_value]['out_correct'].to_list()[0]
            except:
                continue
            if method_type=='dfp' and len(matching_sol_names) < 2 :
                continue

            matching_sol_names=tuple(matching_sol_names)
            result_df.loc[len(result_df)] = [pid,input_name,out_value,matching_sol_names,input_valid,input_valid_byref,out_correct]
            
    return result_df


def compute_res_df(df_triger:pd.DataFrame,num_of_ref_progs:int,method_type:str):
    assert(method_type in ['tc','dfp'])
    res_df=pd.DataFrame(columns=['pid','total','TP','FP'])
    ori_n=num_of_ref_progs
    for pid in df_triger['pid'].unique():
        # print(f"{pid} start")
        num_of_ref_progs=ori_n
        df_triger_pid=df_triger[df_triger['pid']==pid].copy()
        sols_set = set()
        df_triger_pid['sol_names'].apply(lambda x: sols_set.update( [sol_name.split('_')[1] for sol_name in x] ))
        sols_list=sorted(list(sols_set))
        total_sols_num=df_triger_pid['total_sols_num'].max()
        if total_sols_num<num_of_ref_progs:
            num_of_ref_progs=total_sols_num
        total=Cnk(total_sols_num,num_of_ref_progs)

        all_sols_name=['num0','num1','num2','num3','num4','num5','num6','num7','num8','num9']
        out_sols_num=total_sols_num-len(sols_list)
        if out_sols_num!=0:
            out_sols_name=set(all_sols_name)-set(sols_list)
            out_sols_name=list(out_sols_name)
            out_sols_name.sort()
            out_sols_list=out_sols_name[:out_sols_num]
        else:
            out_sols_list=[]
        to_use_sols_name=sols_list+out_sols_list

        combos=list(combinations(to_use_sols_name,num_of_ref_progs))
        combos.sort()

        if method_type=='dfp':
            tp,fp=0,0
            # pick num_of_ref_progs sols and check whether they are in the same group
            # compute the average tp and fp among all inputs

            for combo in combos:
                combo=list(combo)
        
                df_triger_pid['combo_all_true']=df_triger_pid[combo].all(axis=1)
                df_tmp=df_triger_pid[df_triger_pid['combo_all_true']]
                if len(df_tmp)<1:
                    continue
                tp+= len(df_tmp[df_tmp['final_valid']==True])/len(df_tmp)
                fp+= 1-len(df_tmp[df_tmp['final_valid']==True])/len(df_tmp)
                if fp<0:
                    print(f"NOW TP:{tp},FP:{fp}")
                    print(len(df_tmp),len(df_tmp[df_tmp['final_valid']==True]),len(df_tmp[df_tmp['final_valid']==False]))
                    
            if tp==0 and fp==0:
                continue
            res_df.loc[len(res_df)]=[pid,total,tp,fp]
        
        elif method_type=='tc':
            tp,fp=0,0
            for combo in combos:
                
                combo=list(combo)    
                df_triger_pid['combo_any_true']=df_triger_pid[combo].any(axis=1)
                # first find all triger sol groups
                df_tmp=df_triger_pid[df_triger_pid['combo_any_true']].copy()
                if len(df_tmp)<1:
                    continue
                
                df_tmp['sols_in_combo_num'] = df_tmp.loc[:,combo].sum(axis=1)
                max_sols_in_combo = df_tmp.groupby('input_name')['sols_in_combo_num'].transform('max')
                df_tmp=df_tmp[df_tmp['sols_in_combo_num']==max_sols_in_combo]
                tp+=len(df_tmp[df_tmp['final_valid']==True])/len(df_tmp)
                fp+= 1 - len(df_tmp[df_tmp['final_valid']==True])/len(df_tmp)
            if tp==0 and fp==0:
                continue
            res_df.loc[len(res_df)]=[pid,total,tp,fp]

    
    res_df['TP_rate']=res_df['TP']/res_df['total']
    res_df['FP_rate']=res_df['FP']/res_df['total']
    res_df['precision']=res_df['TP']/(res_df['TP']+res_df['FP'])
    return res_df


def ep_get_ref_df(json_path:str):
    df = pd.DataFrame(columns=['task_id', 'input_name', 'ref_out', 'input_valid'])
    with open(json_path,"r") as f:
        ref_json=json.load(f)
    for task_key in ref_json:
        this_task_json=ref_json[task_key]
        task_id=task_key.replace('/','_')
        inp_len=len(this_task_json)
        new_rows=[]
        for i in range(inp_len):
            #print(f"{task_id} input_{i}")
            valid=this_task_json[i][0]
            output=this_task_json[i][1]
            if valid==True:
                new_row=[task_id,f"input_{i}",output,True]
            else:
                new_row=[task_id,f"input_{i}",None,False]
            new_rows.append(new_row)
        df_new_rows=pd.DataFrame(new_rows, columns=['task_id', 'input_name', 'ref_out', 'input_valid'])
        df=pd.concat([df,df_new_rows],ignore_index=True)
    return df

def to_hashable(obj):
    if isinstance(obj, (list, tuple)):
        return tuple(to_hashable(item) for item in obj)
    elif isinstance(obj, dict):
        return frozenset((key, to_hashable(value)) for key, value in obj.items())
    else:
        return obj
    
def ep_get_trigger_df(df:pd.DataFrame,method_type:str):
    assert method_type in ['dfp','tc']
    df['out']=df['out'].apply(to_hashable)
    result_df=pd.DataFrame(columns=['task_id', 'input_name','out','sol_names','input_valid','out_correct'])
    A=df[(df['sol_name']=='put')]
    B=df[~(df['sol_name']=='put')]
    A_outputs=A[['task_id', 'input_name', 'out']].drop_duplicates()
    B_grouped = (B.groupby(['task_id', 'input_name']))
    totoal_len=len(B_grouped)
    count=0
    for (task_id, input_name), group_df in B_grouped:
        count+=1
        if count%1000==0:
            print(f"get_triger: {count}/{totoal_len}")
        group_df=group_df.drop_duplicates(subset=['task_id','input_name','sol_name','out'])
        out_values = group_df['out'].values
        unique_out_values = set(out_values) 
        for out_value in unique_out_values:
            a_out_df=A_outputs[(A_outputs['task_id'] == task_id) & (A_outputs['input_name'] == input_name)]
            if len(a_out_df)<1 :
                continue
            a_out = A[(A['task_id'] == task_id) & (A['input_name'] == input_name)]['out'].values[0]
            if out_value==a_out:
                continue
            matching_sol_names = group_df[group_df['out']==out_value]['sol_name'].to_list()
            input_valid=group_df[group_df['out']==out_value]['input_valid'].to_list()[0]
            out_correct=group_df[group_df['out']==out_value]['out_correct'].to_list()[0]
            if method_type=='dfp' and len(matching_sol_names) < 2 :
                continue
            matching_sol_names=tuple(matching_sol_names)
            result_df.loc[len(result_df)] = [task_id,input_name,out_value,matching_sol_names,input_valid,out_correct]
    result_df['len_sol_names']=result_df['sol_names'].apply(lambda x:len(x))
    result_df['final_valid']= ( (result_df['input_valid'] == True) & (result_df['out_correct']) )
    result_df['total_sols_num']=result_df['task_id'].apply(lambda x:len(df[df['task_id']==x].drop_duplicates(subset=['sol_name']))-1)
    for i in range(10):
        column_name=f"num{i}"
        result_df[column_name] = result_df['sol_names'].apply(lambda x: 1 if f"sol{i}" in x else 0)
    return result_df


def ep_compute_res(df_triger:pd.DataFrame,num_of_ref_progs:int,method_type:str):
    assert method_type in ['dfp','tc']
    res_df=pd.DataFrame(columns=['task_id','total','TP','FP'])
    ori_n=num_of_ref_progs
    for task_id in df_triger['task_id'].unique():
        num_of_ref_progs=ori_n
        df_triger_pid=df_triger[df_triger['task_id']==task_id].copy()
        sols_set = set()
        df_triger_pid['sol_names'].apply(lambda x: sols_set.update( [f"num{sol_name[-1]}" for sol_name in x] ))
        sols_list=sorted(list(sols_set))
        total_sols_num=df_triger_pid['total_sols_num'].max()
        if total_sols_num<num_of_ref_progs:
            num_of_ref_progs=total_sols_num
        total=Cnk(total_sols_num,num_of_ref_progs)
        all_sols_name=['num0','num1','num2','num3','num4','num5','num6','num7','num8','num9']
        out_sols_num=total_sols_num-len(sols_list)
        if out_sols_num!=0:
            out_sols_name=set(all_sols_name)-set(sols_list)
            out_sols_name=list(out_sols_name)
            out_sols_name.sort()
            out_sols_list=out_sols_name[:out_sols_num]
        else:
            out_sols_list=[]    
        to_use_sols_name=sols_list+out_sols_list

        combos=list(combinations(to_use_sols_name,num_of_ref_progs))
        combos.sort()
        if method_type=='dfp':
            tp,fp=0,0
            for combo in combos:
                combo=list(combo)
                df_triger_pid['combo_all_true']=df_triger_pid[combo].all(axis=1)
                df_tmp=df_triger_pid[df_triger_pid['combo_all_true']]
                if len(df_tmp)<1:
                    continue
                tp+= len(df_tmp[df_tmp['final_valid']==True])/len(df_tmp)
                fp+= 1-len(df_tmp[df_tmp['final_valid']==True])/len(df_tmp)
                if fp<0:
                    print(f"NOW TP:{tp},FP:{fp}")
                    print(len(df_tmp),len(df_tmp[df_tmp['final_valid']==True]),len(df_tmp[df_tmp['final_valid']==False]))
                    
            if tp==0 and fp==0:
                continue
            res_df.loc[len(res_df)]=[task_id,total,tp,fp]
        elif method_type=='tc':
            tp,fp=0,0
            for combo in combos:
                combo=list(combo)    
                df_triger_pid['combo_any_true']=df_triger_pid[combo].any(axis=1)
                df_tmp=df_triger_pid[df_triger_pid['combo_any_true']].copy()
                if len(df_tmp)<1:
                    continue
                
                df_tmp['sols_in_combo_num'] = df_tmp.loc[:,combo].sum(axis=1)
                max_sols_in_combo = df_tmp.groupby('input_name')['sols_in_combo_num'].transform('max')
                df_tmp=df_tmp[df_tmp['sols_in_combo_num']==max_sols_in_combo]
                tp+=len(df_tmp[df_tmp['final_valid']==True])/len(df_tmp)
                fp+= 1 - len(df_tmp[df_tmp['final_valid']==True])/len(df_tmp)
            if tp==0 and fp==0:
                continue
            res_df.loc[len(res_df)]=[task_id,total,tp,fp]
        else:
            raise RuntimeError(f"Wrong method_type: {method_type}")
    res_df['TP_rate']=res_df['TP']/res_df['total']
    res_df['FP_rate']=res_df['FP']/res_df['total']
    res_df['precision']=res_df['TP']/(res_df['TP']+res_df['FP'])
    return res_df


def ep_can_compute_res(df_triger:pd.DataFrame,num_of_ref_progs:int,method_type:str):
    assert method_type in ['dfp','tc']
    res_df=pd.DataFrame(columns=['task_id','total','TP','FP','FP_bad_input'])
    ori_n=num_of_ref_progs
    for task_id in df_triger['task_id'].unique():
        num_of_ref_progs=ori_n
        df_triger_pid=df_triger[df_triger['task_id']==task_id].copy()
        sols_set = set()
        df_triger_pid['sol_names'].apply(lambda x: sols_set.update( [f"num{sol_name[-1]}" for sol_name in x] ))
        sols_list=sorted(list(sols_set))
        total_sols_num=df_triger_pid['total_sols_num'].max()
        if total_sols_num<num_of_ref_progs:
            num_of_ref_progs=total_sols_num
        total=Cnk(total_sols_num,num_of_ref_progs)
        all_sols_name=['num0','num1','num2','num3','num4','num5','num6','num7','num8','num9']
        out_sols_num=total_sols_num-len(sols_list)
        if out_sols_num!=0:
            out_sols_name=set(all_sols_name)-set(sols_list)
            out_sols_name=list(out_sols_name)
            out_sols_name.sort()
            out_sols_list=out_sols_name[:out_sols_num]
        else:
            out_sols_list=[]    
        to_use_sols_name=sols_list+out_sols_list

        combos=list(combinations(to_use_sols_name,num_of_ref_progs))
        combos.sort()
        if method_type=='dfp':
            tp,fp=0,0
            fp_bad_input=0
            for combo in combos:
                combo=list(combo)
                df_triger_pid['combo_all_true']=df_triger_pid[combo].all(axis=1)
                df_tmp=df_triger_pid[df_triger_pid['combo_all_true']]
                if len(df_tmp)<1:
                    continue
                tp+= len(df_tmp[df_tmp['final_valid']==True])/len(df_tmp)
                fp+= 1-len(df_tmp[df_tmp['final_valid']==True])/len(df_tmp)
                fp_bad_input+=len(df_tmp[df_tmp['input_valid']!=True])/len(df_tmp)
                if fp<0:
                    print(f"NOW TP:{tp},FP:{fp}")
                    print(len(df_tmp),len(df_tmp[df_tmp['final_valid']==True]),len(df_tmp[df_tmp['final_valid']==False]))
                    
            if tp==0 and fp==0:
                continue
            res_df.loc[len(res_df)]=[task_id,total,tp,fp,fp_bad_input]
        elif method_type=='tc':
            tp,fp=0,0
            fp_bad_input=0
            for combo in combos:
                combo=list(combo)    
                df_triger_pid['combo_any_true']=df_triger_pid[combo].any(axis=1)
                df_tmp=df_triger_pid[df_triger_pid['combo_any_true']].copy()
                if len(df_tmp)<1:
                    continue
                df_tmp['sols_in_combo_num'] = df_tmp.loc[:,combo].sum(axis=1)
                max_sols_in_combo = df_tmp.groupby('input_name')['sols_in_combo_num'].transform('max')
                df_tmp=df_tmp[df_tmp['sols_in_combo_num']==max_sols_in_combo]
                tp+=len(df_tmp[df_tmp['final_valid']==True])/len(df_tmp)
                fp+= 1 - len(df_tmp[df_tmp['final_valid']==True])/len(df_tmp)
                fp_bad_input+=len(df_tmp[df_tmp['input_valid']!=True])/len(df_tmp)
            if tp==0 and fp==0:
                continue
            res_df.loc[len(res_df)]=[task_id,total,tp,fp,fp_bad_input]
        else:
            raise RuntimeError(f"Wrong method_type: {method_type}")
    res_df['TP_rate']=res_df['TP']/res_df['total']
    res_df['FP_rate']=res_df['FP']/res_df['total']
    res_df['precision']=res_df['TP']/(res_df['TP']+res_df['FP'])
    res_df['FP_bad_input_rate']=res_df['FP_bad_input']/res_df['total']
    return res_df

# ----------------------------
# MAIN EXECUTION
# ----------------------------
if __name__ == "__main__":
    # ----------------------------
    # 1. TrickyBugs ë°ì´í„° ë¡œë“œ
    # ----------------------------
    base_path = "/local_datasets/a2024105535/TrickCatcher/Datasets/TrickyBugs"
    problems = load_trickybugs_data_revised(base_path, lang="cpp")

    # ----------------------------
    # 2. ë³€í˜• ì½”ë“œ ìƒì„±
    # ----------------------------
    output_base_path = "/local_datasets/a2024105535/TrickCatcher/Outputs"
    num_variants = 3

    create_variants(problems, output_base_path, k=num_variants)

    '''# ----------------------------
    # 3. í…ŒìŠ¤íŠ¸ ì…ë ¥ ìƒì„±
    # ----------------------------
    num_test_inputs = 10
    generate_buggy_test_inputs(problems, output_base_path, num_inputs=num_test_inputs)

    # ----------------------------
    # 4. ë³€í˜• ì½”ë“œ ê²€ì¦
    # ----------------------------
    verify_variants(problems, output_base_path)

    # ----------------------------
    # 5. í…ŒìŠ¤í¬ ì˜¤ë¼í´ í‰ê°€
    # ----------------------------
    ref_json_path = "/local_datasets/a2024105535/TrickCatcher/Datasets/TrickyBugsRef/ref_outputs.json"
    df_ref = ep_get_ref_df(ref_json_path)

    # GenProgsVerifiedì—ì„œ ê° ë¬¸ì œë³„ ë³€í˜• ì½”ë“œ ì¶œë ¥ ìˆ˜ì§‘
    variant_results = []
    inputs_base_path = Path(output_base_path) / "chat_generated_inputs"
    
    df_variants = task_oracle(problems, str(inputs_base_path), lang="cpp")

    # trigger df ìƒì„±
    df_trigger = ep_get_trigger_df(df_variants, method_type="dfp")

    # ìµœì¢… í‰ê°€
    res_df = ep_compute_res(df_trigger, num_of_ref_progs=2, method_type="dfp")

    print("\nâœ… TrickCatcher í‰ê°€ ì™„ë£Œ!")
    print(res_df.head())
    
    # ê²°ê³¼ ì €ì¥
    result_save_path = Path(output_base_path) / "evaluation_results.csv"
    res_df.to_csv(result_save_path, index=False)
    print(f"ğŸ“Š í‰ê°€ ê²°ê³¼ ì €ì¥: {result_save_path}")'''